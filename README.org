#+title: MNIST from scratch
#+description: Using cuda to fit MNIST

[[file:acc.png]]

* Acknowledgements
This project is inspired by the YouTube series on GPU programming by [[https://www.youtube.com/playlist?list=PL5XwKDZZlwaY7t0M5OLprpkJUIrF8Lc9j][Simon Oz]]. Most of the kernels are based on the didactic examples he provides. Thank you Simon!

* Forward pass
:PROPERTIES:
:header-args:C++: :noeval :tangle no :main no
:END:

This forward pass expects a matrix:

$$X \in \mathbb{R}^{bs \times n}$$

where bs is the batch size and n is the number of features. The weights matrix is:

$$W \in \mathbb{R}^{n \times out\_w}$$

where out_w is the number of output neurons. The biases matrix is:

$$B \in \mathbb{R}^{1 \times out\_w}$$

The output matrix is:

$$O \in \mathbb{R}^{bs \times out\_w}$$

The calculation is:

$$O = X \cdot W + B$$

#+begin_src C++
// input of (bs, n) matrix representing bs amount of samples where each sample has n dimentions.
__global__ void forward(int bs, int n, int out_w,
			float* input, float* weights, float* biases, float* output) {
  // y for rows (height of the mat)
  int row = blockIdx.y * blockDim.y + threadIdx.y; 
  // x for column (width of the mat)
  int column = blockIdx.x * blockDim.x + threadIdx.x; 

  // do the dot product between the row and col
  if (row < bs && column < out_w) {
    output[row*out_w + column] = biases[column];
    for (int i = 0; i < n; i++) {
      output[row * out_w + column] += weights[i * out_w + column] * input[row * n + i];
    }
  }
}
#+end_src

Now we need an activation function. We will use the relu function:

$$relu(x) = max(0, x)$$

#+begin_src C++
__global__ void relu(int w, int h, float* input, float* output) {
  int row = blockIdx.y * blockDim.y + threadIdx.y; 
  int column = blockIdx.x * blockDim.x + threadIdx.x; 
  if (row < h && column < w) {
    float act = input[row * w + column];
    output[row * w + column] = act > 0.f ? act : 0.f; // relu part
  }
}
#+end_src


Finally to output the logits at the end we need softmax:

$$\text{softmax}(x) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$$

To mitigate overflow we can substract the maxium input vector from the exponents the powers are then always negative

$$\text{softmax}(x) = \frac{e^{x_i - \max(x)}}{\sum_{j=1}^{n} e^{x_j - \max(x)}}$$

#+begin_src C++
__global__ void softmax(int w, int h, float* input, float* output) {
  int row = blockIdx.y * blockDim.y + threadIdx.y; 
  int column = blockIdx.x * blockDim.x + threadIdx.x; 
  if (row < h && column < w) {
    float maxin = input[row * w + 0];
    for (int i = 1; i < w; i++) {
      maxin = max(maxin, input[row * w + i]);
    }
    float div = 0.f;
    for (int i = 0; i < w; i++) {
      div += exp(input[row * w + i] - maxin);
    }
    output[row * w + column] = exp(input[row * w + column] - maxin) / div;
  }
}
#+end_src

Now having the output probabilities we can calculate the loss. We will use the cross entropy loss:

$$\text{cross entropy}(y, \hat{y}) = -\sum_{i=1}^{n} y_i \log(\hat{y}_i)$$

where$$y$$is the true label and$$\hat{y}$$is the predicted label. We also use max in the y hat part to avoid log(0)

: we could use atomics but i am not sure how to implement them yet
#+begin_src C++
// gt for groud truth
// input is a matrix of batch_size x num_classes
// the kernel loops of the the number of classes per item in the batch
__global__ void cross_entropy(int w, int h, float* preds, float* gt, float* output) { 
  int idx = blockIdx.x*blockDim.x + threadIdx.x; // get the index of the current thread
  if (idx < h) {
    float loss = 0.f;
    for (int i = 0; i < w; i++) { // loop over the number of classes
      loss -= gt[idx * w + i] * log(max(1e-6, preds[idx * w + i]));
    }
    output[idx] = loss;
  }
}
#+end_src

Finally for initialising the weights we can use kaiming he initialisation:

$$\text{he init}(w, h) = \sqrt{\frac{2}{w}} \cdot \text{randn}$$

where randn is a random number from a normal distribution. Not going in depth but this is done to avoid internal covariate shift.
#+begin_src C++
__global__ void init_rand(int w, int h, float* weights) {
  int row = blockIdx.y * blockDim.y + threadIdx.y; 
  int column = blockIdx.x * blockDim.x + threadIdx.x; 
  if (row < h && column < w) {
    curandState state; // State for the random number generator
    curand_init(42, row * w + column, 0, &state); // Initialize the state
    weights[row * w + column] = sqrtf(2.0 / w) * curand_normal(&state);
  }
}
#+end_src

* Backward pass
:PROPERTIES:
:header-args:C++: :noeval :tangle "./kernels/bw.cu" :main no
:END:

$$ x^n = a^{n-1}W^n+b^n $$

This means for layer n the activations of layer n-1 is equal to its inputs "x^n"

The backward pass involves gradient calculation. By applying the chain rule we can back-propagate the error. Given the loss function lets calculate the backwards cross entropy:

$$\mathcal{L} = \text{cross entropy}(y, \hat{y})$$

Lets start with the following equation with is the derivative of the loss with respect to the weights of the *last* layer:

$$\frac{\partial \mathcal{L}}{\partial w} = \frac{\partial \mathcal{L}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial w}$$

This equation says that the derivative of the loss with respect to the weights of the previous layer is the derivative of the loss with respect to the output of the last layer times the derivative of the output of the last layer with respect to the weights of the last layer. Lets start derivating!

The final activation is the softmax function. Lets derivate it:
$$\hat{y} = \text{softmax}(x) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$$
$$\log(\hat{y}) = \log(\frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}})$$
$$\log(\hat{y}) = \log(e^{x_i}) - \log(\sum_{j=1}^{n} e^{x_j})$$
$$\log(\hat{y}) = x_i - \log(\sum_{j=1}^{n} e^{x_j})$$
Now taking the derivative actually leads to a very simple result:
$$\frac{\partial \log(\hat{y})}{\partial x_k} = \delta_{ik} - \hat{y}_k$$
$$\frac{1}{\hat{y}} \frac{\partial \log(\hat{y})}{\partial x_k} = \delta_{ik} - \hat{y}_k$$
$$\frac{\partial \hat{y}}{\partial x} = \hat{y}(1 - \hat{y})$$

The delta function is 1 if i equals k and 0 otherwise. This is the derivative of the softmax function.

Then we applied a cross entropy loss function. Lets derivate it:
$$\mathcal{L} = -\sum_{i=1}^{n} y_i \log(\hat{y}_i)$$
$$\frac{\partial \mathcal{L}}{\partial \hat{y}} = -\frac{y}{\hat{y}}$$

Now we can use the product of these two to find the full derivative:
TODO ( I just realised its not w is x )
$$\frac{\partial \mathcal{L}}{\partial w} = \frac{\partial \mathcal{L}}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial w}$$
$$\frac{\partial \mathcal{L}}{\partial w} = -\frac{y}{\hat{y}} \times \hat{y}(1 - \hat{y}) $$ - I am not sure if this is the derivation (double check when possible)
$$\frac{\partial \mathcal{L}}{\partial w} = \hat{y} - y $$ - I do know this is the final


So this is the backwards for the cross entropy:
#+begin_src C++
__global__ void ce_back(int w, int h, float* preds, float* gt, float* output) {
  int row = blockIdx.y * blockDim.y + threadIdx.y; 
  // x for column (width of the mat)
  int column = blockIdx.x * blockDim.x + threadIdx.x; 
  if (row < h && column < w) {
    // $$\frac{\partial \mathcal{L}}{\partial w} = \hat{y} - y $$

    output[row * w + column] = preds[row * w + column] - gt[row * w + column];
   }
}
#+end_src


With the derivate of the loss with respect to the inputs of the output layer: (in here y hat is the activation's of last layer, but from now on i will refer to activation's as a)
$$\frac{\partial \mathcal{L}}{\partial x^n} = \frac{\partial \mathcal{L}}{\partial \hat{y}^n}\frac{\partial \hat{y}^n}{\partial x^n}$$
We must take a step backwards to layer n-1:
$$\frac{\partial \mathcal{L}}{\partial a^{n-1}} = \frac{\partial \mathcal{L}}{\partial \hat{y}^n}\frac{\partial \hat{y}^n}{\partial x^n} \times \frac{\partial x^n}{\partial a^{n-1}}$$

So the values $x^n$: refer to this
$$ x^n = a^{n-1}W^n+b^n $$
$$ \frac{\partial x^n}{\partial a^{n-1}} = W^n $$

We must matrix multiply to backprop. Once we have the derivative of x^n with respect to the loss at the last layer we can go back:
$$ \frac{\partial \mathcal{L}}{\partial x^{n-1}} = \frac{\partial \mathcal{L}}{\partial x^{n}} \frac{\partial x^n}{\partial a^{n-1}} $$
$$ \frac{\partial \mathcal{L}}{\partial x^{n-1}} = \frac{\partial \mathcal{L}}{\partial x^{n}} W^n $$

#+begin_src C++
__global__ void backward(int bs, int n, int out_w, float* weights, float* biases, float* d_l, float* out_d_l) {
  int row = blockIdx.y * blockDim.y + threadIdx.y; 
  int column = blockIdx.x * blockDim.x + threadIdx.x; 
  if (row < bs && column < n) {
    float dl = 0.f;
    // $$ \frac{\partial \mathcal{L}}{\partial x^{n-1}} = \frac{\partial \mathcal{L}}{\partial x^{n}} W^n $$
    // in english our weights times the derivative of the next layer so n + 1
    for (int i = 0; i < n; i++) {
      float w = weights[i * out_w + column];
      dl += w * d_l[row * n + i];
    }
    out_d_l[row * out_w + column] = dl;
  }
}
#+end_src


Finally we need the backprop relu:
#+begin_src C++
__global__ void relu_backwards(int w, int h, float* a, float* d_l, float* b) {
  int row = blockIdx.y * blockDim.y + threadIdx.y; 
  int column = blockIdx.x * blockDim.x + threadIdx.x; 
  if (row < h && column < w) {
    float act = a[row * w + column];
    b[row * w + column] = act > 0.f ? d_l[row * w + column] : 0.f;
  }
}
#+end_src

With this we are just left to calculate the derivative of the loss with respect to the weights:
$$ x^n = a^{n-1}W^n+b^n $$
$$ \frac{\partial x^n}{\partial W^n} = a^{n-1} $$
$$ \frac{\partial x^n}{\partial b^n} = 1 $$

And we can update our weights and biases as follows:
$$ w \leftarrow w - \frac{\eta}{bs}\frac{\partial L}{\partial w^n} $$
$$ b \leftarrow b - \frac{\eta}{bs}\frac{\partial L}{\partial b^n} $$

#+begin_src C++
__global__ void update_layer(int w, int h, int bs, float lr, float* weights, float* biases, float* activations, float* d_l) {
  int row = blockIdx.y * blockDim.y + threadIdx.y; 
  int column = blockIdx.x * blockDim.x + threadIdx.x; 
  if (row < h && column < w) {
    float dw = 0.f;
    float db = 0.f;
    for (int i = 0; i < bs ; i++) {
      float act = activations[i * h + row];
      float dl = d_l[i * w + column];
      dw += act * dl;
      db += dl;
    }
    weights[row * w + column] -= lr * dw / bs;
    biases[column] -= lr * db / bs;
  }
}
#+end_src
