#+title: MNIST from scratch
#+description: Using cuda to fit MNIST

* Acknowledgements
This project is inspired by the YouTube series on GPU programming by [[https://www.youtube.com/playlist?list=PL5XwKDZZlwaY7t0M5OLprpkJUIrF8Lc9j][Simon Oz]]. Most of the kernels are based on the didactic examples he provides. Thank you Simon!

* Main
#+begin_src C++ :tangle "src/main.cpp" :includes '(<iostream.h>)
"hellom"
#+end_src

* Data structures
#+begin_src C++ :tangle "include/network.h" :includes '(<vector>) :main no
#pragma once

class NN {
private:
  std::vector<int> layers;

  // host mem
  std::vector<float*> w;
  std::vector<float*> b;

  // device mem
  std::vector<float*> d_w;
  std::vector<float*> d_b;
  std::vector<float*> d_a;
  std::vector<float*> d_deltas;
#+end_src

* Forward pass
:PROPERTIES:
:header-args:C++: :noeval :tangle "./kernels/fw.cu" :main no
:END:

This forward pass expects a matrix:
$$X \in \mathbb{R}^{bs \times n}$$
where bs is the batch size and n is the number of features. The weights matrix is:
$$W \in \mathbb{R}^{n \times out\_w}$$
where out_w is the number of output neurons. The biases matrix is:
$$B \in \mathbb{R}^{1 \times out\_w}$$
The output matrix is:
$$O \in \mathbb{R}^{bs \times out\_w}$$
The calculation is:
$$O = X \cdot W + B$$
#+begin_src C++
// input of (bs, n) matrix representing bs amount of samples where each sample has n dimentions.
__global__ void forward(int bs, int n, int out_w,
			float* input, float* weights, float* biases, float* out) {
  // y for rows (height of the mat)
  int row = blockIdx.y * blockDim.y + threadIdx.y; 
  // x for column (width of the mat)
  int column = blockIdx.x * blockDim.x + threadIdx.x; 

  // do the dot product between the row and col
  if (row < bs && col < out_w) {
    output[row*out_w + column] = biases[column];
    for (int i = 0; i < n; i++) {
      output[row * out_w + column] += weights[i * out_w + column] * input[row * n + i]
    }
  }
}
#+end_src

Now we need an activation function. We will use the relu function:
$$relu(x) = max(0, x)$$
#+begin_src C++

__global__ void relu(int w, int h, float* input, float* output) {
  int row = blockIdx.y * blockDim.y + threadIdx.y; 
  int column = blockIdx.x * blockDim.x + threadIdx.x; 
  if (row < h && column < w) {
    float act = input[row * w + column];
    output[row * w + column] = act > 0.f ? act : 0.f; // relu part
  }
}
#+end_src


Finally to output the logits at the end we need softmax:
$$\text{softmax}(x) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}$$
To mitigate overflow we can substract the maxium input vector from the exponents the powers are then always negative
$$\text{softmax}(x) = \frac{e^{x_i - \max(x)}}{\sum_{j=1}^{n} e^{x_j - \max(x)}}$$
#+begin_src C++
__global__ void softmax(int w, int h, float* input, float* output) {
  int row = blockIdx.y * blockDim.y + threadIdx.y; 
  int column = blockIdx.x * blockDim.x + threadIdx.x; 
  if (row < h && column < w) {
    float maxin = input[row * w + 0];
    for (int i = 1; i < w; i++) {
      maxin = max(maxin, input[row * w + i]);
    }
    float div = 0.f;
    for (int i = 0; i < w; i++) {
      div += exp(input[row * w + i] - maxin);
    }
    output[row * w + column] = exp(input[row * w + column] - maxin) / div;
  }
}
#+end_src

Now having the output probabilities we can calculate the loss. We will use the cross entropy loss:
$$\text{cross entropy}(y, \hat{y}) = -\sum_{i=1}^{n} y_i \log(\hat{y}_i)$$
where$$y$$is the true label and$$\hat{y}$$is the predicted label. We also use max in the y hat part to avoid log(0)

: we could use atomics but i am not sure how to implement them yet
#+begin_src C++
// gt for groud truth
// input is a matrix of batch_size x num_classes
// the kernel loops of the the number of classes per item in the batch
__global__ void cross_entropy(int w, int h, float* preds, float* gt, float* output) { 
  int idx = blockIdx.x*blockDim.x + threadIdx.x; // get the index of the current thread
  if (idx < h) {
    float loss = 0.f;
    fot (int i = 0; i < w; i++) { // loop over the number of classes
      loss -= gt[idx * w + i] * log(max(1e-6, preds[idx * w + i]));
    }
    outputs[idx] = loss;
  }
}
#+end_src

Finally for initialising the weights we can use kaiming he initialisation:

$$\text{he init}(w, h) = \sqrt{\frac{2}{w}} \cdot \text{randn}$$

where randn is a random number from a normal distribution. Not going in depth but this is done to avoid internal covariate shift.
#+begin_src C++
__global__ void he_init(int w, int h, float* weights) {
  int row = blockIdx.y * blockDim.y + threadIdx.y; 
  int column = blockIdx.x * blockDim.x + threadIdx.x; 
  if (row < h && column < w) {
    curandState state; // State for the random number generator
    curand_init(42, row * w + column, 0, &state); // Initialize the state
    weights[row * w + column] = sqrtf(2.0 / w) * curand_normal(&state);
  }
}
#+end_src

* Backwards
:PROPERTIES:
:header-args:C++: :noeval :tangle "./kernels/fw.cu" :main no
:END:

